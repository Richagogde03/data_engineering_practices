{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df557de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 12:09:57 WARN Utils: Your hostname, developer resolves to a loopback address: 127.0.1.1; using 192.168.29.61 instead (on interface enp3s0)\n",
      "25/11/11 12:09:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/11 12:09:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Divvy_Trips_2020_Q1.csv from /home/developer/Downloads/data_engineering_practices_1/Exercises/Exercise-6/data/Divvy_Trips_2020_Q1.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 12:10:07 WARN TaskSetManager: Stage 0 contains a task of very large size (17517 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/11 12:10:13 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "25/11/11 12:10:14 WARN TaskSetManager: Stage 1 contains a task of very large size (17517 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Divvy_Trips_2019_Q4.csv from /home/developer/Downloads/data_engineering_practices_1/Exercises/Exercise-6/data/Divvy_Trips_2019_Q4.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 12:10:20 WARN TaskSetManager: Stage 2 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/11 12:10:24 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 2 (TID 5): Attempting to kill Python Worker\n",
      "25/11/11 12:10:24 WARN TaskSetManager: Stage 3 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 3:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 2 CSV files from ZIPs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import zipfile\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads all CSV files from ZIP archives in the data folder into a list of Spark DataFrames.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.appName(\"Exercise6\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "    base_dir = os.getcwd()\n",
    "    zip_folder = os.path.join(base_dir, \"data\", \"*.zip\")\n",
    "    report_dir = os.path.join(base_dir, \"reports\")\n",
    "\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    zip_files = glob.glob(zip_folder)\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for zip_path in zip_files:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            for file_name in z.namelist():\n",
    "\n",
    "                # skip Mac system files\n",
    "                if file_name.startswith(\"__MACOSX\") or file_name.startswith(\"._\"):\n",
    "                    continue\n",
    "\n",
    "                if file_name.endswith(\".csv\"):\n",
    "                    print(f\"Reading {file_name} from {zip_path}\")\n",
    "                    with z.open(file_name) as f:\n",
    "                        data = f.read().decode(\"utf-8\")\n",
    "                        rdd = spark.sparkContext.parallelize(data.splitlines())\n",
    "                        df = spark.read.csv(rdd, header=True, inferSchema=True)\n",
    "                        all_dfs.append(df)\n",
    "\n",
    "    print(f\"\\nLoaded {len(all_dfs)} CSV files from ZIPs.\")\n",
    "    return all_dfs\n",
    "\n",
    "\n",
    "# def main():\n",
    "all_dfs = load_data()\n",
    "df_2019_Q4 = all_dfs[0]\n",
    "df_2020_Q1 = all_dfs[1]\n",
    "\n",
    "# print(\"First ZIP DataFrame:\")\n",
    "# df_2019_Q4.show(5, truncate=False)\n",
    "\n",
    "# print(\"Second ZIP DataFrame:\")\n",
    "# df_2020_Q1.show(5, truncate=False)\n",
    "# return all_dfs\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092c827",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07396754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 12:10:36 WARN TaskSetManager: Stage 4 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/11 12:10:43 WARN TaskSetManager: Stage 12 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 12:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|date      |avg_trip_duration |\n",
      "+----------+------------------+\n",
      "|2019-10-01|519.1034563470391 |\n",
      "|2019-10-02|489.1325637447672 |\n",
      "|2019-10-03|507.1099269445638 |\n",
      "|2019-10-04|502.3697794462694 |\n",
      "|2019-10-05|529.6975832789027 |\n",
      "|2019-10-06|540.2795057520239 |\n",
      "|2019-10-07|515.5357023690357 |\n",
      "|2019-10-08|515.4543694020819 |\n",
      "|2019-10-09|512.1497682738434 |\n",
      "|2019-10-10|505.07017693819984|\n",
      "+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "df = df_2020_Q1.withColumn(\"date\", F.to_date(F.col(\"start_time\")))\n",
    "\n",
    "avg_trip_duration_per_day = (\n",
    "    df.groupBy(\"date\")\n",
    "      .agg(F.avg(\"tripduration\").alias(\"avg_trip_duration\"))\n",
    "      .orderBy(\"date\")\n",
    ")\n",
    "avg_trip_duration_per_day.toPandas().to_csv(\"reports/avg_trip_per_day.csv\", index=False)\n",
    "avg_trip_duration_per_day.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0be04b2",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c4fcf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 12:11:01 WARN TaskSetManager: Stage 15 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/11 12:11:05 WARN TaskSetManager: Stage 23 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 23:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|date      |total_trips|\n",
      "+----------+-----------+\n",
      "|2019-10-01|18425      |\n",
      "|2019-10-02|9882       |\n",
      "|2019-10-03|15647      |\n",
      "|2019-10-04|14570      |\n",
      "|2019-10-05|10452      |\n",
      "|2019-10-06|13396      |\n",
      "|2019-10-07|17256      |\n",
      "|2019-10-08|17537      |\n",
      "|2019-10-09|17226      |\n",
      "|2019-10-10|15795      |\n",
      "+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df_with_date = df_2020_Q1.withColumn(\"date\", F.to_date(F.col(\"start_time\")))\n",
    "\n",
    "trips_per_day = (\n",
    "    df_with_date.groupBy(\"date\")\n",
    "    .agg(F.count(\"*\").alias(\"total_trips\"))\n",
    "    .orderBy(\"date\")\n",
    ")\n",
    "trips_per_day.toPandas().to_csv(\"reports/trips_per_day.csv\", index=False)\n",
    "trips_per_day.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86052f66",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ea3b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 12:11:50 WARN TaskSetManager: Stage 26 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/11 12:11:54 WARN TaskSetManager: Stage 39 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 39:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+----------+----+\n",
      "|month|from_station_name  |trip_count|rank|\n",
      "+-----+-------------------+----------+----+\n",
      "|10   |Canal St & Adams St|6564      |1   |\n",
      "|11   |Canal St & Adams St|3445      |1   |\n",
      "|12   |Canal St & Adams St|2928      |1   |\n",
      "+-----+-------------------+----------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "df_with_month = df_2020_Q1.withColumn(\"month\", F.month(F.col(\"start_time\")))\n",
    "\n",
    "station_count = df_with_month.groupby(\"month\", \"from_station_name\").agg(F.count(\"*\").alias(\"trip_count\"))\n",
    "\n",
    "window_spec = Window.partitionBy(\"month\").orderBy(F.col(\"trip_count\").desc())\n",
    "ranked_stations = (\n",
    "    station_count\n",
    "    .withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "    .filter(F.col(\"rank\") == 1)\n",
    "    .orderBy(\"month\")\n",
    ")\n",
    "ranked_stations.toPandas().to_csv(\"reports/most_popular_start_station.csv\", index=False)\n",
    "ranked_stations.show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5458370",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f2c79e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 12:12:10 WARN TaskSetManager: Stage 45 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/11 12:12:13 WARN TaskSetManager: Stage 48 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/11 12:12:17 WARN TaskSetManager: Stage 61 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 61:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------+----------+----+\n",
      "|start_date|from_station_name           |trip_count|rank|\n",
      "+----------+----------------------------+----------+----+\n",
      "|2019-12-17|Canal St & Adams St         |153       |1   |\n",
      "|2019-12-17|Clinton St & Madison St     |144       |2   |\n",
      "|2019-12-17|Clinton St & Washington Blvd|124       |3   |\n",
      "|2019-12-18|Canal St & Adams St         |123       |1   |\n",
      "|2019-12-18|Clinton St & Madison St     |115       |2   |\n",
      "|2019-12-18|Clinton St & Washington Blvd|94        |3   |\n",
      "|2019-12-19|Canal St & Adams St         |133       |1   |\n",
      "|2019-12-19|Clinton St & Madison St     |123       |2   |\n",
      "|2019-12-19|Clinton St & Washington Blvd|95        |3   |\n",
      "|2019-12-20|Canal St & Adams St         |131       |1   |\n",
      "|2019-12-20|Clinton St & Washington Blvd|109       |2   |\n",
      "|2019-12-20|Clinton St & Madison St     |94        |3   |\n",
      "|2019-12-21|Streeter Dr & Grand Ave     |63        |1   |\n",
      "|2019-12-21|Kingsbury St & Kinzie St    |47        |2   |\n",
      "|2019-12-21|Wells St & Concord Ln       |46        |3   |\n",
      "|2019-12-22|Shedd Aquarium              |87        |1   |\n",
      "|2019-12-22|Lake Shore Dr & Monroe St   |79        |2   |\n",
      "|2019-12-22|Streeter Dr & Grand Ave     |70        |3   |\n",
      "|2019-12-23|Canal St & Adams St         |109       |1   |\n",
      "|2019-12-23|Clinton St & Madison St     |87        |2   |\n",
      "+----------+----------------------------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df_2020_Q1.withColumn(\"start_date\", F.to_date(F.col(\"start_time\")))\n",
    "max_date = df.select(F.max(\"start_date\")).collect()[0][0]\n",
    "\n",
    "two_weeks_ago = F.date_sub(F.lit(max_date),14)\n",
    "\n",
    "df_last_two_weeks = df.filter(F.col(\"start_date\") >= two_weeks_ago).alias(\"df_last_two_weeks\")\n",
    "\n",
    "station_counts = (\n",
    "    df_last_two_weeks.groupBy(\"start_date\", \"from_station_name\")\n",
    "    .agg(F.count(\"*\").alias(\"trip_count\"))\n",
    ")\n",
    "\n",
    "window_spec = Window.partitionBy(\"start_date\").orderBy(F.desc(\"trip_count\"))\n",
    "\n",
    "top_3_stations_per_day = (\n",
    "    station_counts\n",
    "    .withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "    .filter(F.col(\"rank\") <= 3)\n",
    "    .orderBy(\"start_date\", \"rank\")\n",
    ")\n",
    "\n",
    "top_3_stations_per_day.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"reports/top_3_trip_stations_last_two_weeks.csv\")\n",
    "\n",
    "top_3_stations_per_day.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae741b00",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c5dccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 12:12:31 WARN TaskSetManager: Stage 67 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/11 12:12:35 WARN TaskSetManager: Stage 70 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 70:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|gender|avg_trip_duration|\n",
      "+------+-----------------+\n",
      "|Female|509.8082474784837|\n",
      "|  Male|478.6205057415161|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_avg = df_2020_Q1.filter(F.isnotnull(F.col(\"gender\"))).groupBy(\"gender\").agg(F.avg(\"tripduration\").alias(\"avg_trip_duration\"))\n",
    "\n",
    "total_avg.toPandas().to_csv(\"reports/male_or_female.csv\", header=True, index=False)\n",
    "total_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd171b",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64df7f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 12:12:51 WARN TaskSetManager: Stage 73 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/11 12:12:53 WARN TaskSetManager: Stage 74 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/11 12:12:57 WARN TaskSetManager: Stage 76 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/11 12:12:59 WARN TaskSetManager: Stage 77 contains a task of very large size (24035 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 77:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+\n",
      "|age|tripduration|trip_type|\n",
      "+---+------------+---------+\n",
      "|35 |999.0       |Longest  |\n",
      "|33 |999.0       |Longest  |\n",
      "|58 |999.0       |Longest  |\n",
      "|29 |999.0       |Longest  |\n",
      "|28 |999.0       |Longest  |\n",
      "|25 |999.0       |Longest  |\n",
      "|30 |999.0       |Longest  |\n",
      "|69 |999.0       |Longest  |\n",
      "|31 |999.0       |Longest  |\n",
      "|28 |999.0       |Longest  |\n",
      "|58 |1,000.0     |Shortest |\n",
      "|28 |1,000.0     |Shortest |\n",
      "|44 |1,000.0     |Shortest |\n",
      "|32 |1,000.0     |Shortest |\n",
      "|63 |1,000.0     |Shortest |\n",
      "|32 |1,000.0     |Shortest |\n",
      "|23 |1,000.0     |Shortest |\n",
      "|40 |1,000.0     |Shortest |\n",
      "|37 |1,000.0     |Shortest |\n",
      "|26 |1,000.0     |Shortest |\n",
      "+---+------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "age = df_2020_Q1.withColumn(\"age\", F.lit(2020) - F.col(\"birthyear\"))\n",
    "age = age.filter((F.col(\"age\") > 0) & (F.col(\"age\") < 100))\n",
    "\n",
    "longest_trip = age.orderBy(F.desc(\"tripduration\")).select(\"age\", \"tripduration\").limit(10).withColumn(\"trip_type\", F.lit(\"Longest\"))\n",
    "\n",
    "shortest_trip = age.orderBy(F.asc(\"tripduration\")).select(\"age\", \"tripduration\").limit(10).withColumn(\"trip_type\", F.lit(\"Shortest\"))\n",
    "\n",
    "combined = longest_trip.union(shortest_trip)\n",
    "combined.toPandas().to_csv(\"reports/top_10_ages.csv\", index=False)\n",
    "\n",
    "combined.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
